{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b509ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV\n",
    "path = \"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/select_repos/webapps_large_scan.csv\"\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Stable sort so rows with type == 'webapp' come first, keep all rows/columns, preserve original order within groups\n",
    "is_webapp = df[\"type\"].astype(str).str.strip().str.lower().eq(\"webapp\").astype(int)\n",
    "df_sorted = df.assign(_is_webapp=is_webapp).sort_values(\n",
    "    by=\"_is_webapp\", ascending=False, kind=\"mergesort\"\n",
    ").drop(columns=\"_is_webapp\")\n",
    "\n",
    "# Write back to the same file\n",
    "df_sorted.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "749b82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new entries in the second file: 0\n",
      "\n",
      "Sample of new repo names:\n",
      "[]\n",
      "Number of entries in the first file but not in the second: 35\n",
      "\n",
      "Sample of missing repo names:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "path_first = \"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/select_repos/webapps_large_scan.csv\"\n",
    "path_second = \"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/select_repos/webapps_large_scan_second_try.csv\"\n",
    "\n",
    "# Read both files\n",
    "df_first = pd.read_csv(path_first)\n",
    "df_second = pd.read_csv(path_second)\n",
    "\n",
    "# Get sets of repo names\n",
    "first_repos = set(df_first[\"repo_full_name\"].dropna().str.strip())\n",
    "second_repos = set(df_second[\"repo_full_name\"].dropna().str.strip())\n",
    "\n",
    "# Find new ones\n",
    "new_repos = second_repos - first_repos\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of new entries in the second file: {len(new_repos)}\")\n",
    "print(\"\\nSample of new repo names:\")\n",
    "print(list(new_repos)[:10])  # show up to 10\n",
    "\n",
    "# Find repos missing in the second file\n",
    "missing_repos = first_repos - second_repos\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of entries in the first file but not in the second: {len(missing_repos)}\")\n",
    "print(\"\\nSample of missing repo names:\")\n",
    "#print(list(missing_repos)[:10])  # show up to 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a5d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'webapp' rows in the second file: 72\n",
      "Number of 'webapp' rows missing from the second file: 2\n",
      "['BerriAI/litellm', 'PrefectHQ/prefect']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "path_first = \"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/select_repos/webapps_large_scan.csv\"\n",
    "path_second = \"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/select_repos/webapps_large_scan_second_try.csv\"\n",
    "\n",
    "# Read both files\n",
    "df_first = pd.read_csv(path_first)\n",
    "df_second = pd.read_csv(path_second)\n",
    "\n",
    "# Normalize repo names for reliable comparison\n",
    "df_first[\"repo_full_name\"] = df_first[\"repo_full_name\"].str.strip()\n",
    "df_second[\"repo_full_name\"] = df_second[\"repo_full_name\"].str.strip()\n",
    "\n",
    "# 1Ô∏è‚É£ Count rows in the second file where type == 'webapp'\n",
    "webapps_in_second = (df_second[\"type\"].astype(str).str.lower() == \"webapp\").sum()\n",
    "\n",
    "# 2Ô∏è‚É£ Find rows in the first file that are NOT in the second and have type == 'webapp'\n",
    "missing_repos = set(df_first[\"repo_full_name\"]) - set(df_second[\"repo_full_name\"])\n",
    "missing_webapps = df_first[df_first[\"repo_full_name\"].isin(missing_repos) & (df_first[\"type\"].astype(str).str.lower() == \"webapp\")]\n",
    "\n",
    "# Results\n",
    "print(f\"Number of 'webapp' rows in the second file: {webapps_in_second}\")\n",
    "print(f\"Number of 'webapp' rows missing from the second file: {len(missing_webapps)}\")\n",
    "print(missing_webapps[\"repo_full_name\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc2c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Summary statistics for numeric columns:\n",
      "                      min       max      mean\n",
      "contributors        10.00    3910.0    307.30\n",
      "stars             5104.00  178887.0  20963.78\n",
      "python_pct          50.62     100.0     83.32\n",
      "commits_last_90d    10.00    4793.0    268.78\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your file\n",
    "path_second = \"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/select_repos/webapps_large_scan_second_try.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(path_second)\n",
    "\n",
    "# Columns to analyze\n",
    "cols = [\"contributors\", \"stars\", \"python_pct\", \"commits_last_90d\"]\n",
    "\n",
    "# Convert columns to numeric (in case they were read as strings)\n",
    "df[cols] = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Compute stats\n",
    "summary = df[cols].agg([\"min\", \"max\", \"mean\"]).T.round(2)\n",
    "\n",
    "# Display result\n",
    "print(\"üìä Summary statistics for numeric columns:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f744b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median: 157.5, 75th percentile: 324.0, 90th percentile: 701.6000000000008\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "q50 = contributors.median()\n",
    "q75 = contributors.quantile(0.75)\n",
    "q90 = contributors.quantile(0.9)\n",
    "print(f\"Median: {q50}, 75th percentile: {q75}, 90th percentile: {q90}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "161a831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Percentile Summary (Stars, Commits, Python %):\n",
      "                   Median  75th percentile  90th percentile\n",
      "stars             12553.0         23412.00         42934.70\n",
      "commits_last_90d    107.0           230.25           671.10\n",
      "python_pct           88.5            98.44            99.85\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your second file\n",
    "path_second = \"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/select_repos/webapps_large_scan_second_try.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(path_second)\n",
    "\n",
    "# Ensure numeric columns are actually numeric\n",
    "numeric_cols = [\"stars\", \"commits_last_90d\", \"python_pct\"]\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Compute percentiles\n",
    "percentiles = [0.5, 0.75, 0.9]\n",
    "summary = df[numeric_cols].quantile(percentiles).T\n",
    "summary.columns = [\"Median\", \"75th percentile\", \"90th percentile\"]\n",
    "summary = summary.round(2)\n",
    "\n",
    "print(\"üìä Percentile Summary (Stars, Commits, Python %):\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
