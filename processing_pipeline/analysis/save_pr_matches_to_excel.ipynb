{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 files, [PosixPath('/Users/danarapp/Desktop/energypattern-keyword-search/data/keywords_2/pr_first_iteration/paperless-ngx.paperless-ngx.v2.18.4.pr_corpus.parquet'), PosixPath('/Users/danarapp/Desktop/energypattern-keyword-search/data/keywords_2/pr_first_iteration/saleor.saleor.3.21.19.pr_corpus.parquet'), PosixPath('/Users/danarapp/Desktop/energypattern-keyword-search/data/keywords_2/pr_first_iteration/netbox-community.netbox.v4.4.1.pr_corpus.parquet')]\n",
      "\n",
      "=== Processing: git_datatransfer ===\n",
      "[dedupe] Removed 41 duplicates (from 121 → 80) on (url, matched_word)\n",
      "[merge] After merging by url: 67 rows\n",
      "\n",
      "=== Processing: git_ui ===\n",
      "[dedupe] Removed 0 duplicates (from 8 → 8) on (url, matched_word)\n",
      "[merge] After merging by url: 6 rows\n",
      "\n",
      "=== Processing: git_codeopt ===\n",
      "[dedupe] Removed 32 duplicates (from 103 → 71) on (url, matched_word)\n",
      "[merge] After merging by url: 65 rows\n",
      "\n",
      "=== Counts (after merge-by-url) ===\n",
      "datatransfer - git: 67\n",
      "UI           - git: 6\n",
      "code_opt     - git: 65\n",
      "----\n",
      "TOTAL git: 138\n",
      "\n",
      "Saved review workbook to: /Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/analysis/analyzed_matches/pr_first_iteration3.xlsx\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from constants.abs_paths import AbsDirPath\n",
    "from pathlib import Path\n",
    "from processing_pipeline.utilities.data_transformation import load_all_files\n",
    "import openpyxl\n",
    "\n",
    "# ==============================\n",
    "# Load single source\n",
    "# ==============================\n",
    "base_dir = Path(AbsDirPath.PR_KEYWORDS_MATCHING)\n",
    "df_git = load_all_files(base_dir)\n",
    "\n",
    "# ==============================\n",
    "# Filter into three categories\n",
    "# ==============================\n",
    "df_git_datatransfer = df_git[df_git[\"qa\"] == \"datatransfer\"]\n",
    "df_git_ui           = df_git[df_git[\"qa\"] == \"UI\"]\n",
    "df_git_codeopt      = df_git[df_git[\"qa\"] == \"code_optimization\"]\n",
    "\n",
    "# ==============================\n",
    "# Output paths\n",
    "# ==============================\n",
    "excel_filename = \"pr_first_iteration2.xlsx\"  # change as needed\n",
    "out_dir = Path(\"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/analysis/analyzed_matches\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / excel_filename\n",
    "\n",
    "# ==============================\n",
    "# Helpers (same logic as before)\n",
    "# ==============================\n",
    "def excel_hyperlink_formula(url: str) -> str:\n",
    "    \"\"\"Return an Excel HYPERLINK() formula that shows the raw URL text and is clickable.\"\"\"\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return \"\"\n",
    "    safe = url.replace('\"', '\"\"')\n",
    "    return f'=HYPERLINK(\"{safe}\", \"{safe}\")'\n",
    "\n",
    "def ensure_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure required columns exist and add row_id (original index).\"\"\"\n",
    "    d = df.copy()\n",
    "    if \"pattern\" not in d.columns:\n",
    "        d[\"pattern\"] = \"\"           # default empty\n",
    "    if \"url\" not in d.columns:\n",
    "        d[\"url\"] = \"\"\n",
    "    if \"commit_url\" not in d.columns:\n",
    "        d[\"commit_url\"] = \"\"        # still supported\n",
    "    if \"comment\" not in d.columns:\n",
    "        d[\"comment\"] = \"\"           # still supported\n",
    "    d[\"row_id\"] = d.index           # keep original index for traceability\n",
    "    return d\n",
    "\n",
    "# Columns to include in the Excel (same as before)\n",
    "review_cols = [\"row_id\", \"matched_word\", \"sentence\", \"source\", \"url\", \"pattern\", \"commit_url\", \"comment\"]\n",
    "\n",
    "# ==============================\n",
    "# Prepare the three sheets\n",
    "# ==============================\n",
    "names = [\n",
    "    \"git_datatransfer\",\n",
    "    \"git_ui\",\n",
    "    \"git_codeopt\",  # exact name requested\n",
    "]\n",
    "\n",
    "dfs = [\n",
    "    df_git_datatransfer,\n",
    "    df_git_ui,\n",
    "    df_git_codeopt,\n",
    "]\n",
    "\n",
    "# Deduplicate each df (by url + matched_word) and report removals\n",
    "for i, df in enumerate(dfs):\n",
    "    before = len(df)\n",
    "    deduped = df.drop_duplicates(subset=[\"url\", \"matched_word\"], keep=\"first\").copy()\n",
    "    removed = before - len(deduped)\n",
    "    dfs[i] = deduped\n",
    "    print(f\"[dedupe] Removed {removed} duplicates (from {before} → {len(deduped)}) in dfs[{i}] ({names[i]})\")\n",
    "\n",
    "# Count deduplicated matches\n",
    "count_git_datatransfer = len(dfs[0])\n",
    "count_git_ui           = len(dfs[1])\n",
    "count_git_codeopt      = len(dfs[2])\n",
    "\n",
    "print(\"\\n=== Counts (deduplicated) ===\")\n",
    "print(f\"datatransfer - git: {count_git_datatransfer}\")\n",
    "print(f\"UI           - git: {count_git_ui}\")\n",
    "print(f\"code_opt     - git: {count_git_codeopt}\")\n",
    "print(\"----\")\n",
    "print(f\"TOTAL git: {count_git_datatransfer + count_git_ui + count_git_codeopt}\")\n",
    "\n",
    "# ==============================\n",
    "# Write Excel with three sheets\n",
    "# ==============================\n",
    "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as writer:\n",
    "    for name, df in zip(names, dfs):\n",
    "        dfx = ensure_schema(df)\n",
    "        out = dfx.reindex(columns=review_cols)\n",
    "\n",
    "        # Make both URL columns clickable with raw URL as text\n",
    "        out[\"url\"] = out[\"url\"].apply(excel_hyperlink_formula)\n",
    "        out[\"commit_url\"] = out[\"commit_url\"].apply(excel_hyperlink_formula)\n",
    "\n",
    "        # Write sheet with explicit, unique name\n",
    "        out.to_excel(writer, sheet_name=name, index=False)\n",
    "\n",
    "        # Freeze header row\n",
    "        ws = writer.sheets[name]\n",
    "        ws.freeze_panes = \"A2\"\n",
    "\n",
    "print(f\"Saved review workbook to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658f4e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing sheet: git_datatransfer ===\n",
      "[dedupe] Removed 0 duplicates (from 1 → 1) on (url, matched_word)\n",
      "\n",
      "=== Processing sheet: git_ui ===\n",
      "[dedupe] Removed 0 duplicates (from 1 → 1) on (url, matched_word)\n",
      "\n",
      "=== Processing sheet: git_codeopt ===\n",
      "[dedupe] Removed 0 duplicates (from 1 → 1) on (url, matched_word)\n",
      "\n",
      "✅ Merged workbook saved to: /Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/analysis/analyzed_matches/pr_first_iteration2_merged.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "\n",
    "# ==============================\n",
    "# Paths\n",
    "# ==============================\n",
    "in_path = Path(\"/Users/danarapp/Desktop/energypattern-keyword-search/processing_pipeline/analysis/analyzed_matches/pr_first_iteration2.xlsx\")\n",
    "out_path = in_path.with_name(\"pr_first_iteration2_merged.xlsx\")\n",
    "\n",
    "# Columns expected / order to keep\n",
    "review_cols = [\"row_id\", \"matched_word\", \"sentence\", \"source\", \"url\", \"pattern\", \"commit_url\", \"comment\"]\n",
    "\n",
    "# ==============================\n",
    "# Helpers\n",
    "# ==============================\n",
    "def ensure_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    # Ensure all expected columns exist\n",
    "    for col in review_cols:\n",
    "        if col not in d.columns:\n",
    "            d[col] = \"\"\n",
    "    return d\n",
    "\n",
    "def _join_unique(series, sep=\", \"):\n",
    "    \"\"\"Join unique, non-empty strings in original order.\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in series:\n",
    "        s = \"\" if pd.isna(x) else str(x).strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if s not in seen:\n",
    "            seen.add(s)\n",
    "            out.append(s)\n",
    "    return sep.join(out)\n",
    "\n",
    "def _join_all(series, sep=\", \"):\n",
    "    \"\"\"Join all values as strings (used for row_id).\"\"\"\n",
    "    return sep.join(str(x) for x in series if pd.notna(x) and str(x).strip() != \"\")\n",
    "\n",
    "def process_sheet(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = ensure_schema(df)\n",
    "\n",
    "    # Step 1: exact de-dupe on (url, matched_word)\n",
    "    before = len(d)\n",
    "    d = d.drop_duplicates(subset=[\"url\", \"matched_word\"], keep=\"first\").copy()\n",
    "    removed = before - len(d)\n",
    "    print(f\"[dedupe] Removed {removed} duplicates (from {before} → {len(d)}) on (url, matched_word)\")\n",
    "\n",
    "    # Step 2: merge by url\n",
    "    merged = (\n",
    "        d.groupby(\"url\", dropna=False, sort=False)\n",
    "         .agg({\n",
    "             # merge ONLY these three as requested\n",
    "             \"matched_word\": _join_unique,\n",
    "             \"row_id\": _join_all,\n",
    "             \"sentence\": _join_unique,\n",
    "             # keep others empty strings\n",
    "             \"source\": lambda s: \"\",\n",
    "             \"pattern\": lambda s: \"\",\n",
    "             \"commit_url\": lambda s: \"\",\n",
    "             \"comment\": lambda s: \"\",\n",
    "         })\n",
    "         .reset_index()\n",
    "    )\n",
    "\n",
    "    # Reorder/ensure columns\n",
    "    merged = merged.reindex(columns=review_cols)\n",
    "    return merged\n",
    "\n",
    "# ==============================\n",
    "# Read all sheets, process, and write new Excel\n",
    "# ==============================\n",
    "with pd.ExcelFile(in_path) as xls:\n",
    "    sheet_names = xls.sheet_names\n",
    "    sheets = {name: xls.parse(name) for name in sheet_names}\n",
    "\n",
    "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as writer:\n",
    "    for name, df in sheets.items():\n",
    "        print(f\"\\n=== Processing sheet: {name} ===\")\n",
    "        out = process_sheet(df)\n",
    "        out.to_excel(writer, sheet_name=name, index=False)\n",
    "        ws = writer.sheets[name]\n",
    "        ws.freeze_panes = \"A2\"\n",
    "\n",
    "print(f\"\\n✅ Merged workbook saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feaebffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danarapp/Desktop/energypattern-keyword-search\n"
     ]
    }
   ],
   "source": [
    "cd /Users/danarapp/Desktop/energypattern-keyword-search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
